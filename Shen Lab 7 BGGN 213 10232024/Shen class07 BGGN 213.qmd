---
title: "Shen Lab 7 BGGN 213, Machine Learning 1"
format: html
author: "Celina Shen -- PID (A16673724)"
---

Before we get into clustering methods, let's make some sample data to cluster, where we know what the answer should be.

To help with this, I will use the `rnorm()` function.

```{r}
hist(rnorm(15000, mean = 3))
```

Bimodal distribution
```{r}
n = 30
x <- c( rnorm(n, mean = 3), rnorm(n, mean = -3))
y <- rev(x)

z <- cbind(x,y)
plot(z)
```

## K-means clustering

The function in base R for k-means clustering is called `kmeans()`.

```{r}
km <- kmeans(z, centers = 2)
km
```

>Q. Print out the cluster membership vector (i.e. our main answer)

```{r}
km$cluster
plot(z, col = c("red", "blue"))
```

Plot with clustering result:

```{r}
plot(z, col=km$cluster)
points(km$centers, col = "blue", pch = 18, cex = 2)
```

> Q. Can you cluster our data in `z` into four clusters please?

```{r}
km4 <- kmeans(z, centers = 4)
plot(z, col=km4$cluster)
points(km4$centers, col = "blue", pch = 15, cex = 2)
```

## Hierarchical clustering

The main function for hierarchical Clustering in base R is called `hclust()`
UNlike `kmeans()` I can not just pass my data as input I first need a distance matrix from my data. 
```{r}
d <- dist(z)
hc<- hclust(d)
hc
```

There is a specific hclust plot() method:

```{r}
plot(hc)
abline(h = 10, col = "red")
```

To get my main clustering result (i.e. the membership vector), I can "cut" my tree at a given height. To do this, I will use the `cutree()` function.

```{r}
grps <- cutree(hc, h=10)
grps
```

# Principal Component Analysis

Principal component analysis (PCA) is a well established "multivariate statistical technique" used to reduce the dimensionality of a complex data set to a more manageable number (typically 2D or 3D). This method is particularly useful for highlighting strong paterns and relationships in large datasets (i.e. revealing major similarities and diferences) that are otherwise hard to visualize.

## PCA of UK food data

Import dataset from UK on food consumption
```{r}
url<- "https://tinyurl.com/UK-foods"
x<- read.csv(url, row.names=1)
```

```{r}
##x<- x[,-1]
head(x)
```

Barplot:
```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```

Stacked barplot:

```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))

```
Pairs plot:
```{r}
pairs(as.matrix(x), col=rainbow(nrow(x)))

```

## PCA to the rescue!

The main function to do PCA in base R is called `prcomp()`.
Note that I need to take the transpose of this particular data, as that is what the `prcomp()` help page was asking for.

```{r}
pca <- prcomp(t(x))
summary(pca)
```

Let's see what is inside our result object `pca` that we just calculated:

```{r}
attributes(pca)
```
```{r}
pca$x
```

To make our main result figure, called a "PC plot" (or "score plot", "ordination plot", or "PC1 vs PC2 plot"):

```{r}
plot(pca$x[,1], pca$x[,2], col = c("black", "red", "blue", "darkgreen"),
     pch = 16,xlab = "PC1 (67.4%)", ylab="PC2 (29%")
abline(h=0, col = "gray", lty = 2)
abline(v=0, col = "gray", lty = 2)

```

## Variable loadings plot

Can giv us insight on how the original variables (in this case, the foods), contribute to our new PC axis.

```{r}
## Let's focus on PC1 as it accounts for > 90% variance

par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las=2 )
```




